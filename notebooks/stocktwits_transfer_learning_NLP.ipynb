{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for NLP: Sentiment Analysis on Amazon Reviews\n",
    "In this notebook, we show how transfer learning can be applied to detecting the sentiment of amazon reviews, between positive and negative reviews.\n",
    "\n",
    "This notebook uses the work from [Howard and Ruder, Ulmfit](https://arxiv.org/pdf/1801.06146.pdf).\n",
    "The idea of the paper (and it implementation explained in the [fast.ai deep learning course](http://course.fast.ai/lessons/lesson10.html)) is to learn a language model trained on a very large dataset, e.g. a Wikipedia dump. The intuition is that if a model is able to predict the next word at each word, it means it has learnt something about the structure of the language we are using.\n",
    "\n",
    "[Word2vec](https://arxiv.org/pdf/1310.4546.pdf) and the likes have lead to huge improvements on various NLP tasks. This could be seen as a first step to transfer learning, where the pre-trained word vectors correspond to a transfer of the embedding layer.\n",
    "The ambition of [Ulmfit](https://arxiv.org/pdf/1801.06146.pdf) (and others like [ELMO](https://arxiv.org/pdf/1802.05365.pdf) or the [Transformer language model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) recently introduced) is to progressively move the NLP field to the state where Computer Vision has risen thanks to the ImageNet challenge. Thanks to the ImageNet chalenge, today it is easy to download a model pre-trained on massive dataset of images, remove the last layer and replace it by a classifier or a regressor depending on the interest. \n",
    "\n",
    "With Ulmfit, the goal is for everyone to be able to use a pre-trained language model and use it a backbone which we can use along with a classifier and a regressor. The game-changing apect of transfer learning is that we are no longer limited by the size of trzining data! With only a fraction of the data size that was necessary before, we can trtain a classifier/regressor and have very good result with few labelled data.\n",
    "\n",
    "Given that labelled text data are difficult to get, in comparison with unlabelled text data which is almost infinite, transfer learning is likely to change radically the field of NLP, and help lead to a maturity state closer to computyer vision.\n",
    "\n",
    "The architecture for the language model used in ULMFit is the [AWD-LSTM language model](https://arxiv.org/pdf/1708.02182.pdf) by Merity.\n",
    "\n",
    "While we are using this language model for this experiment, we keep an eye open to a recently proposed character language model with [Contextual String Embedings](http://alanakbik.github.io/papers/coling2018.pdf) by Akbik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrate the power of Ulmfit on a dataset of Amazon reviews available on Kaggle at https://www.kaggle.com/bittlingmayer/amazonreviews/home.\n",
    "We use code from the excellent fastai course and use it for a different dataset. The original code is available at https://github.com/fastai/fastai/tree/master/courses/dl2\n",
    "\n",
    "The data consists of 4M reviews that are either positives or negatives. Training a model with FastText classifier results in a f1 score of 0.916.\n",
    "We show that uing only a fraction of this dataset we are able to reach similar and even better results.\n",
    "\n",
    "We encourage you to try it on your own tasks!\n",
    "Note that if you are interested in Regression instead of classification, you can also do it following this [advice](http://forums.fast.ai/t/regression-using-ulmfit/18063/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is organized as such:\n",
    "\n",
    "- Tokenize the reviews and create dictionaries\n",
    "- Download a pre-trained model and link the dictionary to the embedding layer of the model\n",
    "- Fine-tune the language model on the amaxon reviews texts\n",
    "\n",
    "We have then the backbone of our algorithm: a pre-trained language model fine-tuned on Amazon reviews\n",
    "\n",
    "- Add a classifier to the language model and train the classifier layer only\n",
    "- Gradually defreeze successive layers to train different layers on the amazon reviews\n",
    "- Run a full classification task for several epochs\n",
    "- Use the model for inference!\n",
    "\n",
    "We end this notebook by looking at the specific effect of training size on the overall performance. This is to test the hypothesis that the ULMFit model does not need much labeled data to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you should download the data from https://www.kaggle.com/bittlingmayer/amazonreviews, and put the extracted files into an ./Amazon folder somewher you like, and use this path for this notebook.\n",
    "\n",
    "Also, we recommend working on a dedicated environment (e.g. mkvirtualenv fastai). Then clone the fastai github repo https://github.com/fastai/fastai and install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = '/home/ubuntu/data'\n",
    "root_path = '/home/paperspace/data'\n",
    "path = './data/stocktwits_posts'\n",
    "data_file_nonextension_name = 'stocktwits_preprocessed_20180706T115955Z_20180827'\n",
    "data_file = f'{path}/{data_file_nonextension_name}.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fastai==0.7\n",
    "#! pip install kaggle\n",
    "#! conda install -y spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ln -s \"$root_path\" ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir \"$path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kaggle datasets download -d bittlingmayer/amazonreviews -p \"$path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! bzip2 -d \"$path\"/test.ft.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! bzip2 -d \"$path\"/train.ft.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.lm_rnn import *\n",
    "import html\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \\\n",
    "confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_clean_stocktwits_posts = pd.read_json(\n",
    "    data_file, \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$TSLA love the stock, but playing ER would be ...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>0.1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$FB $AAPL $AMZN $GOOGL $BTC.X $TSLA $MU $BIDU ...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>-0.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$NVDA $AMAT $AMZN $FB $MSFT $GOOGL $AMD $NFLX ...</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Trade Blocks for $TSLA (Largest Trade: 0.4...</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_body     sentiment  \\\n",
       "0  $TSLA love the stock, but playing ER would be ...       Bullish   \n",
       "1  $FB $AAPL $AMZN $GOOGL $BTC.X $TSLA $MU $BIDU ...       Bullish   \n",
       "2                                              $TSLA  Undetermined   \n",
       "3  $NVDA $AMAT $AMZN $FB $MSFT $GOOGL $AMD $NFLX ...  Undetermined   \n",
       "4  Big Trade Blocks for $TSLA (Largest Trade: 0.4...  Undetermined   \n",
       "\n",
       "   vader_sentiment  \n",
       "0           0.1779  \n",
       "1          -0.4466  \n",
       "2           0.0000  \n",
       "3           0.0000  \n",
       "4          -0.2263  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_clean_stocktwits_posts[['clean_body', 'sentiment', 'vader_sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cases = {\n",
    "    'Bullish': 1,\n",
    "    'Bearish': 0,\n",
    "    'Undetermined': 2\n",
    "}\n",
    "\n",
    "def calculate_numeric_label(text_label):\n",
    "    return label_cases[text_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_clean_stocktwits_posts['labels'] = pd_clean_stocktwits_posts['sentiment'].apply(lambda text_label: calculate_numeric_label(text_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_clean_stocktwits_posts = pd_clean_stocktwits_posts.rename({'clean_body': 'text'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>labels</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$TSLA love the stock, but playing ER would be ...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$FB $AAPL $AMZN $GOOGL $BTC.X $TSLA $MU $BIDU ...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$NVDA $AMAT $AMZN $FB $MSFT $GOOGL $AMD $NFLX ...</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Trade Blocks for $TSLA (Largest Trade: 0.4...</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     sentiment  labels  \\\n",
       "0  $TSLA love the stock, but playing ER would be ...       Bullish       1   \n",
       "1  $FB $AAPL $AMZN $GOOGL $BTC.X $TSLA $MU $BIDU ...       Bullish       1   \n",
       "2                                              $TSLA  Undetermined       2   \n",
       "3  $NVDA $AMAT $AMZN $FB $MSFT $GOOGL $AMD $NFLX ...  Undetermined       2   \n",
       "4  Big Trade Blocks for $TSLA (Largest Trade: 0.4...  Undetermined       2   \n",
       "\n",
       "   vader_sentiment  \n",
       "0           0.1779  \n",
       "1          -0.4466  \n",
       "2           0.0000  \n",
       "3           0.0000  \n",
       "4          -0.2263  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_clean_stocktwits_posts[['text', 'sentiment', 'labels', 'vader_sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,\n",
      "                 8,      9,\n",
      "            ...\n",
      "            166647, 166648, 166649, 166650, 166651, 166652, 166653, 166654,\n",
      "            166655, 166656],\n",
      "           dtype='int64', length=166657)\n"
     ]
    }
   ],
   "source": [
    "print(pd_clean_stocktwits_posts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_stocktwits_posts = pd_clean_stocktwits_posts.query('labels!=2').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=91619, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(pd_stocktwits_posts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.75, validate_percent=0, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, test = train_validate_test_split(pd_stocktwits_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data contains 68714 examples\n",
      "The test data contains 22905 examples\n"
     ]
    }
   ],
   "source": [
    "print(f'The train data contains {len(train)} examples')\n",
    "print(f'The test data contains {len(test)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path(path)\n",
    "\n",
    "CLAS_PATH=PATH/'stocktwits_posts_class'\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=PATH/'stocktwits_posts_lm'\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = train[['labels','text']]\n",
    "df_val = test[['labels','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74233</th>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA 3rd lawsuit initated by bull Yeager but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56568</th>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA Elon Himself: “the worst is yet to come”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78701</th>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA This stock is so broken. Bulls better ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89471</th>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA VW offered $30bil to join TSLA was attem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46757</th>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA added some more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22330</th>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA YA'LL RICH PPL HERE BETTER KEEP ELON OUT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91506</th>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA so we need to get over the cave news guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47842</th>\n",
       "      <td>0</td>\n",
       "      <td>$T at 30, $F at 10, $TSLA at 300. Suspence:) I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17038</th>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA I’m just a scalper I just don’t see Bull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80458</th>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA &amp;'Pumpers?&amp;' You were trolling all wknd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text\n",
       "74233       0  $TSLA 3rd lawsuit initated by bull Yeager but ...\n",
       "56568       0  $TSLA Elon Himself: “the worst is yet to come”...\n",
       "78701       0  $TSLA This stock is so broken. Bulls better ge...\n",
       "89471       1  $TSLA VW offered $30bil to join TSLA was attem...\n",
       "46757       1                              $TSLA added some more\n",
       "22330       0  $TSLA YA'LL RICH PPL HERE BETTER KEEP ELON OUT...\n",
       "91506       1  $TSLA so we need to get over the cave news guy...\n",
       "47842       0  $T at 30, $F at 10, $TSLA at 300. Suspence:) I...\n",
       "17038       0  $TSLA I’m just a scalper I just don’t see Bull...\n",
       "80458       1  $TSLA &'Pumpers?&' You were trolling all wknd ..."
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn.to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "# df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "df_trn.to_json(\n",
    "    f'{CLAS_PATH}/{data_file_nonextension_name}_train.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")\n",
    "\n",
    "df_val.to_json(\n",
    "    f'{CLAS_PATH}/{data_file_nonextension_name}_test.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['Bearish', 'Bullish']\n",
    "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_trn['labels'] = 0\n",
    "df_val['labels'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to fine tune the language model so it's ok to take some of the test set in our train data\n",
    "# for the lm fine-tuning\n",
    "# df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "# df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "df_trn.to_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_train.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")\n",
    "\n",
    "df_val.to_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_test.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:labels\n",
      "1:text\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_train.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")# [['text', 'labels']]\n",
    "\n",
    "df_val = pd.read_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_test.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip'\n",
    ")\n",
    "\n",
    "for i, r in enumerate(df_val):\n",
    "    print(f'{i}:{r}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use functions from the fast.ai course to get data\n",
    "\n",
    "chunksize=6000\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    # print(df[n_lbls])\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): \n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        df = r.rename(columns={x:y for x,y in zip(r.columns,range(0,len(r.columns)))})\n",
    "        # print(r)\n",
    "        tok_, labels_ = get_texts(df, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\n",
    "# df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "# df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "df_trn = pd.read_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_train.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip', chunksize=chunksize\n",
    ")# [['text', 'labels']]\n",
    "\n",
    "df_val = pd.read_json(\n",
    "    f'{LM_PATH}/{data_file_nonextension_name}_test.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip', chunksize=chunksize\n",
    ")# [['text', 'labels']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.json.json.JsonReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# This cell can take quite some time if your dataset is large\n",
    "# Run it once and comment it for later use\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once and comment everything but the load statements for later use\n",
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t_up', 111265),\n",
       " ('$', 99242),\n",
       " ('tsla', 70987),\n",
       " ('1', 69683),\n",
       " ('\\n', 68714),\n",
       " ('xbos', 68714),\n",
       " ('xfld', 68714),\n",
       " ('.', 50203),\n",
       " ('the', 27992),\n",
       " ('to', 23149),\n",
       " (',', 20141),\n",
       " ('is', 17428),\n",
       " ('a', 16626),\n",
       " ('!', 16082),\n",
       " ('and', 12584),\n",
       " ('this', 11816),\n",
       " ('i', 10665),\n",
       " ('of', 10561),\n",
       " ('in', 10285),\n",
       " ('it', 9900),\n",
       " ('?', 9392),\n",
       " ('for', 9199),\n",
       " ('&', 8803),\n",
       " ('you', 8273),\n",
       " ('on', 8019)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the most common tokens\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotline', 1),\n",
       " ('3.38', 1),\n",
       " ('entails', 1),\n",
       " ('haha-', 1),\n",
       " ('.longs', 1),\n",
       " ('yang', 1),\n",
       " ('23.5', 1),\n",
       " ('.musk', 1),\n",
       " ('bullsh', 1),\n",
       " ('chipmakers', 1),\n",
       " ('sheered', 1),\n",
       " ('pseudo', 1),\n",
       " ('clammering', 1),\n",
       " ('robot', 1),\n",
       " ('stifle', 1),\n",
       " ('drop-', 1),\n",
       " ('short=', 1),\n",
       " ('duuumb', 1),\n",
       " ('4.22', 1),\n",
       " ('ajit', 1),\n",
       " ('jain', 1),\n",
       " ('301.84', 1),\n",
       " ('afterall', 1),\n",
       " ('yoyo', 1),\n",
       " ('onion', 1)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the least common tokens\n",
    "freq.most_common()[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your vocabulary by keeping only the most common tokens that appears frequently enough\n",
    "# and constrain the size of your vocabulary. We follow here the 60k recommendation.\n",
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)\n",
    "\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13221, 68714)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre trained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:21:29 (81.8 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html’ saved [857/857]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/robots.txt\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2018-08-28 15:21:29 ERROR 404: Not Found.\n",
      "\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/?C=N;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=N;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:21:29 (78.7 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=N;O=D’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/?C=M;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=M;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:21:29 (96.0 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=M;O=A’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/?C=S;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=S;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:21:29 (115 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=S;O=A’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/?C=D;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=D;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:21:29 (121 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=D;O=A’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:21:29--  http://files.fast.ai/models/wt103/bwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/bwd_wt103.h5’\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  21.4MB/s    in 37s     \n",
      "\n",
      "2018-08-28 15:22:07 (11.9 MB/s) - ‘./data/stocktwits_posts/models/wt103/bwd_wt103.h5’ saved [462387687/462387687]\n",
      "\n",
      "--2018-08-28 15:22:07--  http://files.fast.ai/models/wt103/bwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/bwd_wt103_enc.h5’\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  34.1MB/s    in 16s     \n",
      "\n",
      "2018-08-28 15:22:23 (27.8 MB/s) - ‘./data/stocktwits_posts/models/wt103/bwd_wt103_enc.h5’ saved [462387634/462387634]\n",
      "\n",
      "--2018-08-28 15:22:23--  http://files.fast.ai/models/wt103/fwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/fwd_wt103.h5’\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  29.6MB/s    in 14s     \n",
      "\n",
      "2018-08-28 15:22:36 (32.5 MB/s) - ‘./data/stocktwits_posts/models/wt103/fwd_wt103.h5’ saved [462387687/462387687]\n",
      "\n",
      "--2018-08-28 15:22:36--  http://files.fast.ai/models/wt103/fwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/fwd_wt103_enc.h5’\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  34.0MB/s    in 13s     \n",
      "\n",
      "2018-08-28 15:22:49 (34.0 MB/s) - ‘./data/stocktwits_posts/models/wt103/fwd_wt103_enc.h5’ saved [462387634/462387634]\n",
      "\n",
      "--2018-08-28 15:22:49--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4161252 (4.0M) [text/plain]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/itos_wt103.pkl’\n",
      "\n",
      "models/wt103/itos_w 100%[===================>]   3.97M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2018-08-28 15:22:49 (41.2 MB/s) - ‘./data/stocktwits_posts/models/wt103/itos_wt103.pkl’ saved [4161252/4161252]\n",
      "\n",
      "--2018-08-28 15:22:49--  http://files.fast.ai/models/wt103/?C=N;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=N;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:22:50 (135 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=N;O=A’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:22:50--  http://files.fast.ai/models/wt103/?C=M;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=M;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:22:50 (124 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=M;O=D’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:22:50--  http://files.fast.ai/models/wt103/?C=S;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=S;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:22:50 (124 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=S;O=D’ saved [857/857]\n",
      "\n",
      "--2018-08-28 15:22:50--  http://files.fast.ai/models/wt103/?C=D;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘./data/stocktwits_posts/models/wt103/index.html?C=D;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-28 15:22:50 (122 MB/s) - ‘./data/stocktwits_posts/models/wt103/index.html?C=D;O=D’ saved [857/857]\n",
      "\n",
      "FINISHED --2018-08-28 15:22:50--\n",
      "Total wall clock time: 1m 21s\n",
      "Downloaded: 14 files, 1.7G in 1m 20s (22.2 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this cell to download the pre-trained model.\n",
    "# It will be placed into the PATH that you defined earlier.\n",
    "! wget -nH -r -np -P {path} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights of the model\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238462, 400)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the word embedding layer and keep a 'mean word' for unknown tokens\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "enc_wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary on which the pre-trained model was trained\n",
    "# Define an embedding matrix with the vocabulary of our dataset\n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new embedding matrix for the pre-trained model\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learner object to do the fine-tuning\n",
    "# Here we will freeze everything except the embedding layer, so that we can have a better \n",
    "# embedding for unknown words than just the mean embedding on which we initialise it.\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)\n",
    "\n",
    "learner.model.load_state_dict(wgts)\n",
    "\n",
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf246ba4ca147b7932f5f035eb239f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      5.041968   4.713975   0.308568  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 4.71397]), 0.30856804667612758]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run one epoch of fine-tuning \n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and unfreeze everything to later fine-tune the whole model\n",
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c5438fc620448b95ebf0e751cbfe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      3.525703   3.337948   0.427656  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft_after_lr_find')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft_after_lr_find')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee5277450a44d608ea4d0ba5921fe1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      3.772837   3.538827   0.416185  \n",
      "    1      3.352943   3.17047    0.453603                   \n",
      "    2      3.012864   2.885514   0.491399                   \n",
      "    3      2.771227   2.681538   0.523959                   \n",
      "    4      2.559092   2.521284   0.551148                   \n",
      "    5      2.397784   2.397235   0.573595                   \n",
      "    6      2.271078   2.298868   0.592203                   \n",
      "    7      2.174226   2.215298   0.608304                   \n",
      "    8      2.080278   2.148482   0.621696                   \n",
      "    9      1.99816    2.093355   0.63235                    \n",
      "    10     1.935472   2.048272   0.641846                   \n",
      "    11     1.873651   2.014468   0.649343                   \n",
      "    12     1.818698   1.987754   0.654887                   \n",
      "    13     1.794235   1.967544   0.659171                   \n",
      "    14     1.773556   1.957359   0.661337                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.95736]), 0.66133687986821232]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this if you want to highly tune the LM to the Amazon data, with 15 epochs\n",
    "# use_clr controls the shape of the cyclical (triangular) learning rate\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Backbone for further classification!!\n",
    "learner.save('lm1')\n",
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_encoder('lm1_enc')\n",
    "learner.load('lm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going back to classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we spent some time fine-tuning the language model on our Amazon data, let's see if we can classify easily these reviews.\n",
    "As before, some cells should be run once, and then use data loaders for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "# df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "df_trn = pd.read_json(\n",
    "    f'{CLAS_PATH}/{data_file_nonextension_name}_train.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip', chunksize=chunksize\n",
    ")# [['text', 'labels']]\n",
    "\n",
    "df_val = pd.read_json(\n",
    "    f'{CLAS_PATH}/{data_file_nonextension_name}_test.json.gz', \n",
    "    orient='records', \n",
    "    lines=True, compression='gzip', chunksize=chunksize\n",
    ")# [['text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13221"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "In this part, we adopt an unusual train/test hierarchy. While it's common to train on a big dataset and thewn test on a small one, here we wanrt to test the hypothesis that the model can learn with few training data. Hence we take less data for training than for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select here the 'size' first reviews of our dataset\n",
    "# The paper claims that it's possible to achieve very good results with few labeled examples\n",
    "# So let's try with 100 examples for training, and 5000 examples for validation.\n",
    "# We encourage you to try different values to see the effect of data size on performance.\n",
    "trn_size = 100\n",
    "val_size = 5000\n",
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "\n",
    "train = random.sample(list(zip(trn_clas, trn_labels)), trn_size)\n",
    "trn_clas = np.array([item[0] for item in train])\n",
    "trn_labels = np.array([item[1] for item in train])\n",
    "del train\n",
    "\n",
    "validation = random.sample(list(zip(val_clas, val_labels)), val_size)\n",
    "val_clas = np.array([item[0] for item in validation])\n",
    "val_labels = np.array([item[1] for item in validation])\n",
    "del validation\n",
    "\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "\n",
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ccheck that the validation dataset is well balanced so acccuracy is a good metric\n",
    "# We'll also check other metrics usual for binary classification (precision, recall, f1 score)\n",
    "len(trn_labels[trn_labels == 1]) / len(trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_rnn_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-54a4c3a9fb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n\u001b[0m\u001b[1;32m     11\u001b[0m           \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mem_sz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_rnn_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# We define the model, here it a classifier on top of an RNN language model\n",
    "# We load the language model encoder that we fine tuned before\n",
    "# We freeze everything but the last layer, so that we can train the classification layer only.\n",
    "#load the saved weights from before, and freeze everything until the last layer\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]\n",
    "\n",
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')\n",
    "\n",
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(lrs/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one epoch on the classification layer\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.save('clas_0')\n",
    "learn.load('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradually unfreeze another layer to train a bit more parameters than just the classifier layer\n",
    "learn.freeze_to(-2)\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.save('clas_1')\n",
    "learn.load('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze everything and train for a few epochs on the whole set of parameters of the model\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "learn.save('clas_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Nonw, let's play with the model we've just learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]\n",
    "\n",
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')\n",
    "learn.load('clas_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(input_str: str):\n",
    "\n",
    "    # predictions are done on arrays of input.\n",
    "    # We only have a single input, so turn it into a 1x1 array\n",
    "    texts = [input_str]\n",
    "\n",
    "    # tokenize using the fastai wrapper around spacy\n",
    "    tok = [t.split() for t in texts]\n",
    "    # tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "\n",
    "    # turn into integers for each word\n",
    "    encoded = [stoi[p] for p in tok[0]]\n",
    "\n",
    "    idx = np.array(encoded)[None]\n",
    "    idx = np.transpose(idx)\n",
    "    tensorIdx = VV(idx)\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    p = m.forward(tensorIdx)\n",
    "    return np.argmax(p[0][0].data.cpu().numpy())\n",
    "\n",
    "def prediction(texts):\n",
    "    \"\"\"Do the prediction on a list of texts\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        encoded = text\n",
    "        idx = np.array(encoded)[None]\n",
    "        idx = np.transpose(idx)\n",
    "        tensorIdx = VV(idx)\n",
    "        m.eval()\n",
    "        m.reset()\n",
    "        p = m.forward(tensorIdx)\n",
    "        y.append(np.argmax(p[0][0].data.cpu().numpy()))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I like Feedly\"\n",
    "start = time()\n",
    "print(get_sentiment(sentence))\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prediction(list(val_clas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show relevant metrics for binary classification\n",
    "# We encourage you to try training the classifier with different data size and its effect on performance\n",
    "print(f'Accuracy --> {accuracy_score(y, val_labels)}')\n",
    "print(f'Precision --> {precision_score(y, val_labels)}')\n",
    "print(f'F1 score --> {f1_score(y, val_labels)}')\n",
    "print(f'Recall score --> {recall_score(y, val_labels)}')\n",
    "print(confusion_matrix(y, val_labels))\n",
    "print(classification_report(y, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What training size do we need?\n",
    "The language model has already learnt a lot about the syntax. It is very knowledgeable about the context in which words appear in sentences. However, the language model does not contain any notion of [meaning](https://en.wikipedia.org/wiki/Meaning_%28linguistics%29). This problem is well summarised in [Emily Bender's tweet](https://twitter.com/emilymbender/status/1024042044035985408) during a very interesting twiter thread that occur in July around meaning in NLP. A cool summary of this thread can be found in the [Hugging Face](https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e) blogpost. Hence the meaning in language is very likely to be learned through supervision, with the help of ground-truth examples.\n",
    "\n",
    "However, when we perform some NLP tasks, sentiment analysis in our example, both syntax and meaning are important!\n",
    "The idea is that you can save a lot of time by being taught with a lot of blind synatx first, and then learning meaning. Think of when you start learning a complete new field. Well, it is far easier to learn it in your mother tongue than in another language you master less. \n",
    "\n",
    "The big practical gain here is that once you \"know\" a language, you need less supervised examples to learn a new thing! In our example, it means we need less labeled reviews for us to learn a relevant classifier.\n",
    "\n",
    "Let's verify this hypothesis by training a classifier with several training size and see how this size affects the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(trn_size, val_size):\n",
    "\n",
    "    train = random.sample(list(zip(trn_clas, trn_labels)), trn_size)\n",
    "    aux_trn_clas = np.array([item[0] for item in train])\n",
    "    aux_trn_labels = np.array([item[1] for item in train])\n",
    "    del train\n",
    "\n",
    "    validation = random.sample(list(zip(val_clas, val_labels)), val_size)\n",
    "    aux_val_clas = np.array([item[0] for item in validation])\n",
    "    aux_val_labels = np.array([item[1] for item in validation])\n",
    "    del validation\n",
    "\n",
    "\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "    vs = len(itos)\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "    bs = 48\n",
    "\n",
    "    min_lbl = aux_trn_labels.min()\n",
    "    aux_trn_labels -= min_lbl\n",
    "    aux_val_labels -= min_lbl\n",
    "    c=int(aux_trn_labels.max())+1\n",
    "\n",
    "    # Load data in relevant structures\n",
    "    trn_ds = TextDataset(aux_trn_clas, aux_trn_labels)\n",
    "    val_ds = TextDataset(aux_val_clas, aux_val_labels)\n",
    "    trn_samp = SortishSampler(aux_trn_clas, key=lambda x: len(aux_trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(aux_val_clas, key=lambda x: len(aux_val_clas[x]))\n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "\n",
    "    # Define the model and load the backbone lamguage model\n",
    "    md = ModelData(PATH, trn_dl, val_dl)\n",
    "    dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "    m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "    lr=3e-3\n",
    "    lrm = 2.6\n",
    "    lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "    lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "    wd = 1e-7\n",
    "    wd = 0\n",
    "    learn.load_encoder('lm1_enc')\n",
    "\n",
    "    learn.freeze_to(-1)\n",
    "\n",
    "    # Find th learning rate\n",
    "    learn.lr_find(lrs/1000)\n",
    "\n",
    "    # Run one epoch on the classification layer\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    # Save the trained model\n",
    "    learn.save(f'{trn_size}clas_0')\n",
    "    learn.load(f'{trn_size}clas_0')\n",
    "\n",
    "    # Gradually unfreeze another layer to train a bit more parameters than just the classifier layer\n",
    "    learn.freeze_to(-2)\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    # Save the trained model\n",
    "    learn.save(f'{trn_size}clas_1')\n",
    "    learn.load(f'{trn_size}clas_1')\n",
    "\n",
    "    # Unfreeze everything and train for a few epochs on the whole set of parameters of the model\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))\n",
    "\n",
    "    # Save the model\n",
    "    learn.sched.plot_loss()\n",
    "    learn.save(f'{trn_size}clas_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "val_size = 100000\n",
    "for trn_size in [50, 100, 500, 1000, 5000, 10000, 20000, 50000]:\n",
    "    print('#'*50)\n",
    "    print(f'Experiment with training size {trn_size}')\n",
    "    start = time()\n",
    "    experiment(trn_size, val_size)\n",
    "    t = time() - start\n",
    "    print(f'Time cost: {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notebook issues here, you might want to run this cell from a python script..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Lety's see the evollution of the accuracy when we increas the size of the train data.\n",
    "For each training size, we report the best accuracy among the different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_acc = [0.84558, 0.87324, 0.91232, 0.9203, 0.93174, 0.93584, 0.94032, 0.94616]\n",
    "sizes = [50, 100, 500, 1000, 5000, 10000, 20000, 50000]\n",
    "plt.plot(sizes, best_acc)\n",
    "plt.title('Evolution of performance when increasing the training size')\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sizes, best_acc)\n",
    "plt.title('Evolution of performance when increasing the training size, Zoom on the [0-10000] size zone')\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim([0, 10000])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.log(sizes)/np.log(10), best_acc)\n",
    "plt.title('Evolution of performance when increasing the training size, with log scale for size')\n",
    "plt.xlabel('Training size (log)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first observation is, even with 50 samples only, we get a pretty great accuracy of 0.85!\n",
    "- Then we see that the learning progress is very consequent when going from a size of 50 to 1000 samples\n",
    "- The ULMFit beats the reported score from FastText (~0.92) when using 1000 samples only! Note that the reported score from FastText is from a training using the whole training data (3.6M samples)\n",
    "- The accuracy continues to rise when we increase the training size, but with a lower speed. Here the trade-off comes, where you have to decide whether the extra 0.1% in accuracy is worth paying for more labeled data!\n",
    "- From the log-scale graph we might expect even greater results when raining the training size. We have 4.6M training reviews so we could get orders of magnitude more so we could expect reaching 0.95 accuracy or more with the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
